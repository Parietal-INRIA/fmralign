{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Co-smoothing prediction using the Individual Neural Tuning Model\n\nIn this tutorial, we show how to better predict new contrasts for a target\nsubject using many source subjects corresponding contrasts. For this purpose,\nwe create a template to which we align the target subject, using shared information.\nWe then predict new images for the target and compare them to a baseline.\n\nWe mostly rely on Python common packages and on nilearn to handle\nfunctional data in a clean fashion.\n\n\nTo run this example, you must launch IPython via ``ipython\n--matplotlib`` in a terminal, or use ``jupyter-notebook``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the data\nIn this example we use the IBC dataset, which includes a large number of\ndifferent contrasts maps for 12 subjects.\nWe download the images for subjects sub-01, sub-02, sub-04, sub-05, sub-06\nand sub-07 (or retrieve them if they were already downloaded).\nimgs is the list of paths to available statistical images for each subjects.\ndf is a dataframe with metadata about each of them.\nmask is a binary image used to extract grey matter regions.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign.fetch_example_data import fetch_ibc_subjects_contrasts\n\nsub_list = [\"sub-01\", \"sub-02\", \"sub-04\", \"sub-05\", \"sub-06\", \"sub-07\"]\nimgs, df, mask_img = fetch_ibc_subjects_contrasts(sub_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a masker\nWe define a nilearn masker that will be used to handle relevant data.\n  For more information, visit :\n  'https://nilearn.github.io/stable/manipulating_images/masker_objects.html'\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.maskers import NiftiMasker\n\nmasker = NiftiMasker(mask_img=mask_img).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the data\nFor each subject, we will use two series of contrasts acquired during\ntwo independent sessions with a different phase encoding:\nAntero-posterior(AP) or Postero-anterior(PA).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To infer a template for subjects sub-01 to sub-06 for both AP and PA data,\n# we make a list of 4D niimgs from our list of list of files containing 3D images\n\nfrom nilearn.image import concat_imgs\n\ntemplate_train = []\nfor i in range(6):\n    template_train.append(concat_imgs(imgs[i]))\n\n\n# For subject sub-07, we split it in two folds:\n#   - target train: sub-07 AP contrasts, used to learn alignment to template\n#   - target test: sub-07 PA contrasts, used as a ground truth to score predictions\n# We make a single 4D Niimg from our list of 3D filenames\ntarget_train = df[df.subject == \"sub-07\"][df.acquisition == \"ap\"].path.values\ntarget_train = concat_imgs(target_train)\ntarget_train_data = masker.transform(target_train)\ntarget_test = df[df.subject == \"sub-07\"][df.acquisition == \"pa\"].path.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute a baseline (average of subjects)\nWe create an image with as many contrasts as any subject representing for\neach contrast the average of all train subjects maps.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nmasked_imgs = [masker.transform(img) for img in template_train]\naverage_img = np.mean(masked_imgs[:-1], axis=0)\naverage_subject = masker.inverse_transform(average_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a template from the training subjects.\nWe define an estimator using the class TemplateAlignment:\n  * We align the whole brain through multiple local alignments.\n  * These alignments are calculated on a parcellation of the brain in 100 pieces,\n    this parcellation creates group of functionnally similar voxels.\n  * The template is created iteratively, aligning all subjects data into a\n    common space, from which the template is inferred and aligning again to this\n    new template space.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import index_img\n\nfrom fmralign.alignment_methods import IndividualizedNeuralTuning\nfrom fmralign.hyperalignment.piecewise_alignment import PiecewiseAlignment\nfrom fmralign.hyperalignment.regions import compute_parcels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict new data for left-out subject\nWe use target_train data to fit the transform, indicating it corresponds to\nthe contrasts indexed by train_index and predict from this learnt alignment\ncontrasts corresponding to template test_index numbers.\nFor each train subject and for the template, the AP contrasts are sorted from\n0, to 53, and then the PA contrasts from 53 to 106.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_index = range(53)\ntest_index = range(53, 106)\n\ndenoising_data = np.array(masked_imgs)[:, train_index, :]\ntraining_data = np.array(masked_imgs)[:-1]\ntarget_test_masked = np.array(masked_imgs)[:, test_index, :]\n\n\nparcels = compute_parcels(\n    niimg=template_train[0], mask=masker, n_parcels=100, n_jobs=5\n)\ndenoiser = PiecewiseAlignment(n_jobs=5)\ndenoised_signal = denoiser.fit_transform(X=denoising_data, regions=parcels)\ntarget_denoised_data = denoised_signal[-1]\nmodel = IndividualizedNeuralTuning(\n    parcels=parcels,\n)\nmodel.fit(training_data, verbose=False)\nstimulus_ = np.copy(model.shared_response)\n\n# From the denoised data and the stimulus, we can now extract the tuning\n# matrix from sub-07 AP contrasts, and use it to predict the PA contrasts.\ntarget_tuning = model._tuning_estimator(\n    shared_response=stimulus_[train_index], target=target_denoised_data\n)\n\n# We input the mapping image target_train in a list, we could have input more\n# than one subject for which we'd want to predict : [train_1, train_2 ...]\n\n\npred = model._reconstruct_signal(\n    shared_response=stimulus_[test_index], individual_tuning=target_tuning\n)\nprediction_from_template = masker.inverse_transform(pred)\n\n\n# As a baseline prediction, let's just take the average of activations across subjects.\n\nprediction_from_average = index_img(average_subject, test_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Score the baseline and the prediction\nWe use a utility scoring function to measure the voxelwise correlation\nbetween the prediction and the ground truth. That is, for each voxel, we\nmeasure the correlation between its profile of activation without and with\nalignment, to see if alignment was able to predict a signal more alike the ground truth.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign.metrics import score_voxelwise\n\n# Now we use this scoring function to compare the correlation of predictions\n# made from group average and from template with the real PA contrasts of sub-07\n\n\naverage_score = masker.inverse_transform(\n    score_voxelwise(target_test, prediction_from_average, masker, loss=\"corr\")\n)\n\ntemplate_score = masker.inverse_transform(\n    score_voxelwise(target_test, prediction_from_template, masker, loss=\"corr\")\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting the measures\nFinally we plot both scores\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\n\nbaseline_display = plotting.plot_stat_map(\n    average_score, display_mode=\"z\", vmax=1, cut_coords=[-15, -5]\n)\nbaseline_display.title(\"Group average correlation wrt ground truth\")\ndisplay = plotting.plot_stat_map(\n    template_score, display_mode=\"z\", cut_coords=[-15, -5], vmax=1\n)\ndisplay.title(\"INT prediction correlation wt ground truth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that creating a template and aligning a new subject to it yields\na prediction that is better correlated with the ground truth than just using\nthe average activations of subjects.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotting.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}