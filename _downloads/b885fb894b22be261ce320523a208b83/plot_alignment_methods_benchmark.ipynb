{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Alignment methods benchmark (pairwise ROI case).\n\nIn this tutorial, we compare various methods of alignment on a pairwise alignment\nproblem for Individual Brain Charting subjects. For each subject, we have a lot\nof functional informations in the form of several task-based\ncontrast per subject. We will just work here on a ROI.\n\nWe mostly rely on python common packages and on nilearn to handle functional\ndata in a clean fashion.\n\nTo run this example, you must launch IPython via ``ipython --matplotlib`` in\na terminal, or use ``jupyter-notebook``.\n    :depth: 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the data\nIn this example we use the IBC dataset, which include a large number of\ndifferent contrasts maps for 12 subjects.\nWe download the images for subjects sub-01 and sub-02.\nFiles is the list of paths for each subjects.\ndf is a dataframe with metadata about each of them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign.fetch_example_data import fetch_ibc_subjects_contrasts\n\nfiles, df, mask = fetch_ibc_subjects_contrasts([\"sub-01\", \"sub-02\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract a mask for the visual cortex from Yeo Atlas\nFirst, we fetch and plot the complete atlas\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets, plotting\nfrom nilearn.image import load_img, new_img_like, resample_to_img\n\natlas_yeo_2011 = datasets.fetch_atlas_yeo_2011()\natlas = load_img(atlas_yeo_2011.thick_7)\n\n# Select visual cortex, create a mask and resample it to the right resolution\n\nmask_visual = new_img_like(atlas, atlas.get_fdata() == 1)\nresampled_mask_visual = resample_to_img(mask_visual, mask, interpolation=\"nearest\")\n\n# Plot the mask we will use\nplotting.plot_roi(\n    resampled_mask_visual,\n    title=\"Visual regions mask extracted from atlas\",\n    cut_coords=(8, -80, 9),\n    colorbar=True,\n    cmap=\"Paired\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a masker\nWe define a nilearn masker that will be used to handle relevant data.\nFor more information, visit :\n'http://nilearn.github.io/manipulating_images/masker_objects.html'\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.maskers import NiftiMasker\n\nroi_masker = NiftiMasker(mask_img=resampled_mask_visual).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the data\nFor each subject, for each task and conditions, our dataset contains two\nindependent acquisitions, similar except for one acquisition parameter, the\nencoding phase used that was either Antero-Posterior (AP) or\nPostero-Anterior (PA). Although this induces small differences\nin the final data, we will take  advantage of these pseudo-duplicates to\ncreate a training and a testing set that contains roughly the same signals\nbut acquired independently.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The training set, used to learn alignment from source subject toward target:\n# * source train: AP contrasts for subject sub-01\n# * target train: AP contrasts for subject sub-02\n\nsource_train = df[df.subject == \"sub-01\"][df.acquisition == \"ap\"].path.values\ntarget_train = df[df.subject == \"sub-02\"][df.acquisition == \"ap\"].path.values\n\n# The testing set:\n# * source test: PA contrasts for subject one, used to predict\n#   the corresponding contrasts of subject sub-01\n# * target test: PA contrasts for subject sub-02, used as a ground truth\n#   to score our predictions\n\nsource_test = df[df.subject == \"sub-01\"][df.acquisition == \"pa\"].path.values\ntarget_test = df[df.subject == \"sub-02\"][df.acquisition == \"pa\"].path.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choose the number of regions for local alignment\nFirst, as we will proceed to local alignment we choose a suitable number of\nregions so that each of them is approximately 200 voxels wide. Then our\nestimator will first make a functional clustering of voxels based on train\ndata to divide them into meaningful regions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nn_voxels = roi_masker.mask_img_.get_fdata().sum()\nprint(f\"The chosen region of interest contains {n_voxels} voxels\")\nn_pieces = int(np.round(n_voxels / 200))\nprint(f\"We will cluster them in {n_pieces} regions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the estimators, fit them and do a prediction\nOn each region, we search for a transformation R that is either :\n  *  orthogonal, i.e. R orthogonal, scaling sc s.t. ||sc RX - Y ||^2 is minimized\n  *  a ridge regression : ||XR - Y||^2 + alpha *||R||^2 with a L2 penalization\n     on the norm of R.\n  *  the optimal transport plan, which yields the minimal transport cost\n      while respecting the mass conservation constraints. Calculated with\n      entropic regularization.\n  *  we also include identity (no alignment) as a baseline.\nThen for each method we define the estimator fit it, predict the new image and plot\nits correlation with the real signal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign.metrics import score_voxelwise\nfrom fmralign.pairwise_alignment import PairwiseAlignment\n\nmethods = [\"identity\", \"scaled_orthogonal\", \"ridge_cv\", \"optimal_transport\"]\n\nfor method in methods:\n    alignment_estimator = PairwiseAlignment(\n        alignment_method=method, n_pieces=n_pieces, mask=roi_masker\n    )\n    alignment_estimator.fit(source_train, target_train)\n    target_pred = alignment_estimator.transform(source_test)\n\n    # derive correlation between prediction, test\n    method_error = score_voxelwise(\n        target_test, target_pred, masker=roi_masker, loss=\"corr\"\n    )\n\n    # plot correlation for each method\n    aligned_score = roi_masker.inverse_transform(method_error)\n    title = f\"Correlation of prediction after {method} alignment\"\n    display = plotting.plot_stat_map(\n        aligned_score,\n        display_mode=\"z\",\n        cut_coords=[-15, -5],\n        vmax=1,\n        title=title,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe that all alignment methods perform better than identity\n(no alignment). Ridge is the best performing method, followed by Optimal\nTransport. If you use Ridge though, be careful about the smooth predictions\nit yields.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 5"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}